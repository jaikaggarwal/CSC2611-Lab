{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from nltk import bigrams\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words(\"english\")\n",
    "PUNCTUATION = list(string.punctuation) + [\"``\", \"''\", \"--\"]\n",
    "SYNONYM_PAIRS_FILE = \"synonym_pairs.txt\"\n",
    "ANALOGY_TEST_FILE = \"word-test.v1.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synonym_pairs():\n",
    "    with open(SYNONYM_PAIRS_FILE, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        data = [line.strip().split(\" \") for line in data]\n",
    "        data = {(line[0], line[1]): float(line[2]) for line in data if len(line) == 3}\n",
    "    # Check the number of pairs is correct\n",
    "    assert len(data) == 65\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synonym_pairs():\n",
    "    with open(SYNONYM_PAIRS_FILE, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        data = [line.strip().split(\" \") for line in data]\n",
    "        data = {(line[0], line[1]): float(line[2]) for line in data if len(line) == 3}\n",
    "    # Check the number of pairs is correct\n",
    "    assert len(data) == 65\n",
    "    return data\n",
    "\n",
    "def preprocess_brown_corpus(corpus):\n",
    "#     (word.lower() not in STOPWORDS) and \n",
    "    c = [word.lower() for word in corpus if (word not in PUNCTUATION)]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')\n",
    "brown_corpus = nltk.corpus.brown.words()\n",
    "syn_pairs = load_synonym_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To preprocess the data, I apply the typical preprocessing steps of removing punctuation and converting all words to lower case. I removed all instances of punctuation as they do not aid a model's ability to perform a word similarity task. In addition, since we have no reason to distinguish proper nouns from common nouns in our use case, I lowercase the data so that we have more data to work with (as now words like \"city\" and \"City\" will be treated as the same term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total length of the brown corpus is 1161192 words\n",
      "The total length of the brown corpus is 1013320 words\n"
     ]
    }
   ],
   "source": [
    "# Total length of the corpus\n",
    "print(f\"The total length of the brown corpus is {len(brown_corpus)} words\")\n",
    "preproc_corpus = preprocess_brown_corpus(brown_corpus)\n",
    "print(f\"The total length of the brown corpus is {len(preproc_corpus)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing LSA Model from Basic Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token_counts(corpus):\n",
    "    c = Counter(corpus)\n",
    "    pairs = [(word, c[word]) for word in c]\n",
    "    return sorted(pairs, key=lambda x: x[1])[-1:-5001:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      "the (n=69971)\n",
      "of (n=36412)\n",
      "and (n=28853)\n",
      "to (n=26158)\n",
      "a (n=23195)\n",
      "\n",
      "Least common terms:\n",
      "rourke (n=19)\n",
      "killpath (n=19)\n",
      "haney (n=19)\n",
      "letch (n=19)\n",
      "allen (n=20)\n"
     ]
    }
   ],
   "source": [
    "# Extract frequencies for each token\n",
    "token_counts = extract_token_counts(preproc_corpus)\n",
    "N_most_common = 5\n",
    "print(\"Most common terms:\")\n",
    "for pair in token_counts[:N_most_common]:\n",
    "    print(f\"{pair[0]} (n={pair[1]})\")\n",
    "print(\"\\nLeast common terms:\")\n",
    "for pair in token_counts[-1:-N_most_common-1:-1]:\n",
    "    print(f\"{pair[0]} (n={pair[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in W: 5000\n",
      "Number of words in W: 5031\n"
     ]
    }
   ],
   "source": [
    "W = [pair[0] for pair in token_counts]\n",
    "print(f\"Number of words in W: {len(W)}\")\n",
    "\n",
    "rg_65_words = list(set(sum(syn_pairs, ())))\n",
    "W = list(set(W).union(rg_65_words))\n",
    "print(f\"Number of words in W: {len(W)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1013319/1013319 [01:56<00:00, 8661.04it/s]\n"
     ]
    }
   ],
   "source": [
    "all_bigrams = list(bigrams(preproc_corpus))\n",
    "attested_bigrams = []#defaultdict(lambda: 0)\n",
    "for bigram in tqdm(all_bigrams, total=len(list(all_bigrams))):\n",
    "    w1, w2 = bigram\n",
    "    if w1 in W:\n",
    "        if w2 in W:\n",
    "            attested_bigrams.append((w1, w2))\n",
    "attested_bigrams = Counter(attested_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings(all_words):\n",
    "    token2idx = {all_words[i]: i for i in range(len(all_words))}\n",
    "    idx2token = {val: key for key, val in token2idx.items()}\n",
    "    return token2idx, idx2token\n",
    "\n",
    "def get_cooccurence(word_1, word_2, matrix, token2idx):\n",
    "    idx_1 = token2idx[word_1]\n",
    "    idx_2 = token2idx[word_2]\n",
    "    return matrix[idx_1, idx_2]\n",
    "\n",
    "token2idx, idx2token = get_mappings(list(W))\n",
    "\n",
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "for word_1 in W:\n",
    "    for word_2 in W:\n",
    "        val = attested_bigrams.get((word_1, word_2), 0)\n",
    "        data.append(val)\n",
    "        rows.append(token2idx[word_1])\n",
    "        cols.append(token2idx[word_2])\n",
    "\n",
    "m1 = csr_matrix((data, (rows, cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppmi(model):\n",
    "    joint = (model/model.sum()).todense()\n",
    "    px = joint.sum(axis=0)\n",
    "    py = joint.sum(axis=1)\n",
    "    ind_joint = (px.T@py.T)\n",
    "    div = np.divide(joint, ind_joint, out=np.zeros_like(joint), where=ind_joint!=0)\n",
    "    pmi = np.log2(div)\n",
    "    ppmi = np.where(pmi>0, pmi, 0)\n",
    "    return ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/jai/miniconda3/envs/venv37/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log2\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "ppmi = compute_ppmi(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "for key in syn_pairs:\n",
    "    true.append(syn_pairs[key])\n",
    "    \n",
    "def get_model_preds(model, pair):\n",
    "    w1, w2 = pair\n",
    "    e1 = model[token2idx[w1]].reshape(1, -1)\n",
    "    e2 = model[token2idx[w2]].reshape(1, -1)\n",
    "    pred = cosine_similarity(e1, e2)[0][0]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Contex Vector Correlation:  0.23\n",
      "Fitting 10 dim\n",
      "10 Dimensional LSA Correlation:  0.13\n",
      "Fitting 100 dim\n",
      "100 Dimensional LSA Correlation:  0.31\n",
      "Fitting 300 dim\n",
      "300 Dimensional LSA Correlation:  0.36\n"
     ]
    }
   ],
   "source": [
    "m1_preds = [get_model_preds(m1, pair) for pair in syn_pairs]\n",
    "print(f\"Word-Contex Vector Correlation: \", np.round(pearsonr(true, m1_preds)[0], 2))\n",
    "\n",
    "print(\"Fitting 10 dim\")\n",
    "pca_10 = PCA(n_components=10)\n",
    "m2_10 = pca_10.fit_transform(ppmi)\n",
    "m2_10_preds = [get_model_preds(m2_10, pair) for pair in syn_pairs]\n",
    "print(f\"10 Dimensional LSA Correlation: \", np.round(pearsonr(true, m2_10_preds)[0], 2))\n",
    "\n",
    "print(\"Fitting 100 dim\")\n",
    "pca_100 = PCA(n_components=100)\n",
    "m2_100 = pca_100.fit_transform(ppmi)\n",
    "m2_100_preds = [get_model_preds(m2_100, pair) for pair in syn_pairs]\n",
    "print(f\"100 Dimensional LSA Correlation: \", np.round(pearsonr(true, m2_100_preds)[0], 2))\n",
    "\n",
    "print(\"Fitting 300 dim\")\n",
    "pca_300 = PCA(n_components=300)\n",
    "m2_300 = pca_300.fit_transform(ppmi)\n",
    "m2_300_preds = [get_model_preds(m2_300, pair) for pair in syn_pairs]\n",
    "print(f\"300 Dimensional LSA Correlation: \", np.round(pearsonr(true, m2_300_preds)[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Part 1: Synchronic word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format('/ais/hal9000/datasets/wordembeddings/google_news_w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Using gensim, extract embeddings of words in Table 1 of RG65 that also appeared in\n",
    "the set Wfrom the earlier exericse, i.e., the pairs of words should be identical in all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_embedding(words, model):\n",
    "    word2embedding = {}\n",
    "    for word in words:\n",
    "        word2embedding[word] = model[word]\n",
    "    return word2embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding = word_to_embedding(rg_65_words, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Calculate cosine distance between each pair of word embeddings you have extracted,\n",
    "and report the Pearson correlation between word2vec-based and human similarities. [1 point]\n",
    "Comment on this value in comparison to those from LSA and word-context vectors from analyses in the earlier exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity\n",
    "true_vals = []\n",
    "pred_vals = []\n",
    "for pair in syn_pairs:\n",
    "    w1, w2 = pair\n",
    "    w1_e = word2embedding[w1].reshape(1, -1)\n",
    "    w2_e = word2embedding[w2].reshape(1, -1)\n",
    "    cos_sim = cosine_similarity(w1_e, w2_e)[0][0]\n",
    "    \n",
    "    true_vals.append(syn_pairs[pair])\n",
    "    pred_vals.append(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n"
     ]
    }
   ],
   "source": [
    "# Report pearson correlation\n",
    "corr, p_val = pearsonr(true_vals, pred_vals)\n",
    "print(np.round(corr, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation of the Word2Vec embeddings with the human judgements is much higher than that of both the word-context vectors and the LSA embeddings. The correlations are 0.23 for the word-context vectors, and 0.13, 0.31, and 0.36 for the 10, 100, and 300 dimensional LSA models respectively. For the Word2Vec model, our correlation is 0.77. The fact that the Word2Vec model predictions better correlate with human judgements is sensible, as the word-context vectors and the LSA embeddings are derived from the raw count data, where as the Word2Vec model is trained to learn high quality embeddings for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Analogy Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Perform the analogy test based on data here (or as provided) with the pre-trained\n",
    "word2vec embeddings. Report the accuracy on the semantic analogy test and the syntactic\n",
    "analogy test (see Note below). [2 points] \n",
    "\n",
    "Repeat the analysis with LSA vectors (300 dimensions) from the earlier exercise, and commment on the results in comparison to those from word2vec. [1 point]\n",
    "\n",
    "Note: It is expected that the number of entries you could test with LSA\n",
    "would be smaller than that based on word2vec. For a fair comparison, you should consider\n",
    "reporting model accuracies based on the small test set, for both word2vec and LSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must limit the set of analogies to those that use words present in the LSA vocabulary. We can do so with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analogy_test(word_set):\n",
    "    with open(ANALOGY_TEST_FILE, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        data = [line.strip().split(\" \") for line in data]\n",
    "        data = [list(map(str.lower, line)) for line in data if len(line) == 4]\n",
    "        data = [line for line in data if (all(word in word_set for word in line))]\n",
    "    return data\n",
    "\n",
    "analogies = load_analogy_test(set(W))\n",
    "with open(\"parsed_analogies.txt\", \"w\") as file:\n",
    "    for line in analogies:\n",
    "        file.write(\" \".join(line))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we find the subset of analogies that will work in both Word2Vec and our LSA model, we can split them into semantic and syntactic analogy sets based on the original file. Also, to make the LSA model comparable to Word2Vec, we can convert it to a KeyedVectors object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = KeyedVectors(300)\n",
    "lsa.add_vectors(W, m2_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:00<00:00, 2326.16it/s]\n",
      "100%|██████████| 147/147 [00:24<00:00,  5.97it/s]\n",
      "100%|██████████| 2045/2045 [00:00<00:00, 2809.24it/s]\n",
      "100%|██████████| 2045/2045 [05:15<00:00,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def perform_analogy_test(analogies, model):\n",
    "    correct = 0\n",
    "    for i in tqdm(range(len(analogies))):\n",
    "        analogy = analogies[i]\n",
    "        a, b, c, d = analogy\n",
    "        pred = model.most_similar(positive=[b, c], negative=[a])[0][0]\n",
    "        if d == pred:\n",
    "            correct += 1\n",
    "    return correct/len(analogies)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"semantic_analogies.txt\", \"r\") as file:\n",
    "    semantic_analogies = file.readlines()\n",
    "    semantic_analogies = [line.strip().split(\" \") for line in semantic_analogies]\n",
    "lsa_semantic = perform_analogy_test(semantic_analogies, lsa)\n",
    "w2v_semantic = perform_analogy_test(semantic_analogies, w2v_model)\n",
    "\n",
    "with open(\"syntactic_analogies.txt\", \"r\") as file:\n",
    "    syntactic_analogies = file.readlines()\n",
    "    syntactic_analogies = [line.strip().split(\" \") for line in syntactic_analogies]\n",
    "lsa_syntactic = perform_analogy_test(syntactic_analogies, lsa)\n",
    "w2v_syntactic = perform_analogy_test(syntactic_analogies, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LSA on semantic analogies: 0.17\n",
      "Accuracy of LSA on syntactic analogies: 0.08\n",
      "Accuracy of Word2Vec on semantic analogies: 0.66\n",
      "Accuracy of Word2Vec on syntactic analogies: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of LSA on semantic analogies: {np.round(lsa_semantic, 2)}\")\n",
    "print(f\"Accuracy of LSA on syntactic analogies: {np.round(lsa_syntactic, 2)}\")\n",
    "print(f\"Accuracy of Word2Vec on semantic analogies: {np.round(w2v_semantic, 2)}\")\n",
    "print(f\"Accuracy of Word2Vec on syntactic analogies: {np.round(w2v_syntactic, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Suggest a way to improve the existing set of vector-based models in capturing word\n",
    "similarities in general, and provide justifications for your suggestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word similarity tasks, one thing that our current vector models miss out on is the particular sense of the word being used. Through context (the paired word in the Rubenstein & Goodenough (1965) task), humans can determine the sense of the word that is required to make similarity judgements. For instance, take the pair (asylum, madhouse). Here, through the use of the word \"madhouse\", humans can infer we care about the sense of the word \"asylum\" that refers to a psychiatric hospital. However, if we instead showed the pair (asylum, refuge), humans would understand that we are now referring to a different sense of the word \"asylum\" used for political refugees. In both cases, the word \"asylum\" would be quite similar to the paired word.\n",
    "\n",
    "The models we've used thusfar are static, and so have one sense of the word \"asylum\". So, it is possible that our models find one pair to be more similar than the other because a particular word sense dominated its training data. To improve our models, we ought to take context into account. One way to do this would be by using a contextual model such as BERT, and finding a prototypical vector for each of a word's senses using a variety of contextual usages of a word. \n",
    "\n",
    "Referring back to the case above, we could show the model many cases of the word \"asylum\" used in both of its senses, and then create a prototypical vector for each sense cluster. We could then treat these two prototypical sense vectors as distinct vectors for the model and find the embedding that has the highest cosine similarity with the paired word. This would make the vector models more accurate in making word similarity judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Part 1: Diachronic word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\", \"rb\") as file:\n",
    "    diachronic = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = diachronic['w'] #2000\n",
    "decades = diachronic['d']\n",
    "embeddings = diachronic['E'] #num_words X num_decades X dim_size (300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the year of the first embedding available for a given word, as certain words are not available in the first 1-2 decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_of_first_embedding(embeddings):\n",
    "    word_to_first_embedding = {}\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(len(embeddings[i])):\n",
    "            if all(embeddings[i][j]):\n",
    "                word_to_first_embedding[words[i]] = j \n",
    "                break\n",
    "    return word_to_first_embedding\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_most_least_changed(reordered_scores, reordered_words):\n",
    "    zipped = list(zip(reordered_scores, reordered_words))\n",
    "    least = zipped[:20]\n",
    "    most = zipped[-20:]\n",
    "    print(\"MOST\")\n",
    "    for pair in least:\n",
    "        print(f\"Word: {pair[1]} Change: {np.round(pair[0], 3)}\")\n",
    "        \n",
    "    print(\"LEAST\")\n",
    "    for pair in most:\n",
    "        print(f\"Word: {pair[1]} Change: {np.round(pair[0], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_semantic_change(change, words):\n",
    "    words = np.asarray(words)\n",
    "    least_most = np.argsort(change)\n",
    "    reordered_scores = change[least_most]\n",
    "    reordered_words = words[least_most]\n",
    "    report_most_least_changed(reordered_scores, reordered_words)\n",
    "    return reordered_scores, reordered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_displacement(embeddings, words, word_to_first_embedding):\n",
    "    net_change = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        subset = np.asarray(embeddings[idx]).reshape(len(decades), -1)\n",
    "        subset = subset[word_to_first_embedding[words[idx]]:, :]\n",
    "        change_over_time = cosine_similarity(subset, subset)\n",
    "        net_change.append(change_over_time[0][-1])\n",
    "    net_change = np.asarray(net_change)\n",
    "    return net_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_drift(embeddings, words, word_to_first_embedding):\n",
    "    mean_change = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        subset = np.asarray(embeddings[idx]).reshape(len(decades), -1)\n",
    "        subset = subset[word_to_first_embedding[words[idx]]:, :]\n",
    "        dists = []\n",
    "        for i in range(len(subset)):\n",
    "            if (all(subset[i])):\n",
    "                dist = cosine_similarity(subset[i].reshape(1, -1), subset[0].reshape(1, -1))\n",
    "                dists.append(dist[0][0])\n",
    "        mean = np.mean(dists)\n",
    "        mean_change.append(mean)\n",
    "    mean_change = np.asarray(mean_change)\n",
    "    return mean_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_volatility(embeddings, words, word_to_first_embedding):\n",
    "    mean_change = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        subset = np.asarray(embeddings[idx]).reshape(len(decades), -1)\n",
    "        subset = subset[word_to_first_embedding[words[idx]]:, :]\n",
    "        dists = []\n",
    "        for i in range(len(subset)-1):\n",
    "            if (all(subset[i])) and (all(subset[i+1])):\n",
    "                dist = cosine_similarity(subset[i].reshape(1, -1), subset[i+1].reshape(1, -1))\n",
    "                dists.append(dist[0][0])\n",
    "        mean = np.mean(dists)\n",
    "        mean_change.append(mean)\n",
    "    mean_change = np.asarray(mean_change)\n",
    "    return mean_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_first_embedding = get_year_of_first_embedding(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST\n",
      "Word: programs Change: 0.009\n",
      "Word: objectives Change: 0.022\n",
      "Word: computer Change: 0.039\n",
      "Word: radio Change: 0.095\n",
      "Word: sector Change: 0.099\n",
      "Word: goals Change: 0.155\n",
      "Word: approach Change: 0.16\n",
      "Word: van Change: 0.163\n",
      "Word: shri Change: 0.176\n",
      "Word: media Change: 0.187\n",
      "Word: impact Change: 0.2\n",
      "Word: perspective Change: 0.207\n",
      "Word: patterns Change: 0.218\n",
      "Word: berkeley Change: 0.222\n",
      "Word: shift Change: 0.223\n",
      "Word: film Change: 0.224\n",
      "Word: assessment Change: 0.232\n",
      "Word: stanford Change: 0.233\n",
      "Word: challenge Change: 0.234\n",
      "Word: therapy Change: 0.239\n",
      "LEAST\n",
      "Word: miles Change: 0.674\n",
      "Word: payment Change: 0.677\n",
      "Word: door Change: 0.678\n",
      "Word: evening Change: 0.678\n",
      "Word: week Change: 0.68\n",
      "Word: feet Change: 0.681\n",
      "Word: september Change: 0.682\n",
      "Word: god Change: 0.685\n",
      "Word: december Change: 0.686\n",
      "Word: daughter Change: 0.688\n",
      "Word: months Change: 0.69\n",
      "Word: century Change: 0.693\n",
      "Word: january Change: 0.698\n",
      "Word: increase Change: 0.699\n",
      "Word: october Change: 0.703\n",
      "Word: years Change: 0.709\n",
      "Word: february Change: 0.714\n",
      "Word: november Change: 0.717\n",
      "Word: june Change: 0.739\n",
      "Word: april Change: 0.765\n"
     ]
    }
   ],
   "source": [
    "m1 = semantic_displacement(embeddings, words, word_to_first_embedding)\n",
    "m1_scores, m1_words = sort_semantic_change(m1, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST\n",
      "Word: objectives Change: 0.147\n",
      "Word: programs Change: 0.166\n",
      "Word: computer Change: 0.184\n",
      "Word: radio Change: 0.251\n",
      "Word: sector Change: 0.274\n",
      "Word: goals Change: 0.293\n",
      "Word: shri Change: 0.337\n",
      "Word: therapy Change: 0.361\n",
      "Word: van Change: 0.37\n",
      "Word: mcgraw Change: 0.375\n",
      "Word: input Change: 0.379\n",
      "Word: patterns Change: 0.384\n",
      "Word: evaluation Change: 0.389\n",
      "Word: berkeley Change: 0.393\n",
      "Word: jobs Change: 0.401\n",
      "Word: wiley Change: 0.405\n",
      "Word: stanford Change: 0.409\n",
      "Word: wilson Change: 0.418\n",
      "Word: jones Change: 0.421\n",
      "Word: technology Change: 0.43\n",
      "LEAST\n",
      "Word: daughter Change: 0.766\n",
      "Word: payment Change: 0.767\n",
      "Word: century Change: 0.772\n",
      "Word: church Change: 0.773\n",
      "Word: june Change: 0.777\n",
      "Word: december Change: 0.777\n",
      "Word: vessels Change: 0.778\n",
      "Word: years Change: 0.779\n",
      "Word: increase Change: 0.779\n",
      "Word: months Change: 0.779\n",
      "Word: july Change: 0.779\n",
      "Word: feet Change: 0.78\n",
      "Word: trees Change: 0.783\n",
      "Word: february Change: 0.788\n",
      "Word: january Change: 0.791\n",
      "Word: september Change: 0.794\n",
      "Word: miles Change: 0.797\n",
      "Word: november Change: 0.799\n",
      "Word: october Change: 0.801\n",
      "Word: april Change: 0.82\n"
     ]
    }
   ],
   "source": [
    "m2 = semantic_drift(embeddings, words, word_to_first_embedding)\n",
    "m2_scores, m2_words = sort_semantic_change(m2, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST\n",
      "Word: harper Change: 0.578\n",
      "Word: jones Change: 0.578\n",
      "Word: sector Change: 0.592\n",
      "Word: berkeley Change: 0.598\n",
      "Word: wiley Change: 0.598\n",
      "Word: baltimore Change: 0.601\n",
      "Word: martin Change: 0.601\n",
      "Word: princeton Change: 0.602\n",
      "Word: wilson Change: 0.611\n",
      "Word: adams Change: 0.611\n",
      "Word: therapy Change: 0.62\n",
      "Word: johnson Change: 0.62\n",
      "Word: goals Change: 0.621\n",
      "Word: haven Change: 0.625\n",
      "Word: philip Change: 0.625\n",
      "Word: evaluation Change: 0.625\n",
      "Word: j Change: 0.626\n",
      "Word: publications Change: 0.643\n",
      "Word: stanford Change: 0.644\n",
      "Word: maryland Change: 0.648\n",
      "LEAST\n",
      "Word: blood Change: 0.82\n",
      "Word: patients Change: 0.821\n",
      "Word: june Change: 0.822\n",
      "Word: buildings Change: 0.822\n",
      "Word: temperature Change: 0.825\n",
      "Word: october Change: 0.827\n",
      "Word: decrease Change: 0.827\n",
      "Word: july Change: 0.828\n",
      "Word: solution Change: 0.828\n",
      "Word: cent Change: 0.829\n",
      "Word: trees Change: 0.831\n",
      "Word: vessels Change: 0.831\n",
      "Word: university Change: 0.831\n",
      "Word: february Change: 0.833\n",
      "Word: december Change: 0.84\n",
      "Word: january Change: 0.841\n",
      "Word: september Change: 0.844\n",
      "Word: november Change: 0.845\n",
      "Word: miles Change: 0.848\n",
      "Word: april Change: 0.855\n"
     ]
    }
   ],
   "source": [
    "m3 = semantic_volatility(embeddings, words, word_to_first_embedding)\n",
    "m3_scores, m3_words = sort_semantic_change(m3, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Semantic Displacement</th>\n",
       "      <th>Semantic Drift</th>\n",
       "      <th>Semantic Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Semantic Displacement</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900806</td>\n",
       "      <td>0.678416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semantic Drift</th>\n",
       "      <td>0.900806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.789278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semantic Volatility</th>\n",
       "      <td>0.678416</td>\n",
       "      <td>0.789278</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Semantic Displacement  Semantic Drift  \\\n",
       "Semantic Displacement               1.000000        0.900806   \n",
       "Semantic Drift                      0.900806        1.000000   \n",
       "Semantic Volatility                 0.678416        0.789278   \n",
       "\n",
       "                       Semantic Volatility  \n",
       "Semantic Displacement             0.678416  \n",
       "Semantic Drift                    0.789278  \n",
       "Semantic Volatility               1.000000  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_corrs = pd.DataFrame({\"Semantic Displacement\": m1, \"Semantic Drift\": m2, \"Semantic Volatility\": m3}).corr(method='pearson')\n",
    "pearson_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changepoint_detection(test_words):\n",
    "    mean_change = []\n",
    "    for word in test_words:\n",
    "        idx = words.index(word)\n",
    "        subset = np.asarray(embeddings[idx]).reshape(len(decades), -1)\n",
    "        subset = subset[word_to_first_embedding[words[idx]]:, :]\n",
    "        dists = []\n",
    "        most_recent_embedding = subset[0]\n",
    "        for i in range(len(subset)-1):\n",
    "            if (all(most_recent_embedding)) and (all(subset[i+1])):\n",
    "                dist = cosine_similarity(subset[i].reshape(1, -1), subset[i+1].reshape(1, -1))\n",
    "                dists.append(dist[0][0])\n",
    "                most_recent_embedding = subset[i+1]\n",
    "            else:\n",
    "                if len(dists) == 0:\n",
    "                    dists.append(1)\n",
    "                else:\n",
    "                    dists.append(dists[-1])\n",
    "        mean_change.append(dists)\n",
    "    return mean_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"computer\", \"program\", \"objectives\"]\n",
    "mean_change = changepoint_detection(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4d7d981d90>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(decades[1:], mean_change[0], label=test_words[0])\n",
    "plt.plot(decades[1:], mean_change[1], label=test_words[1])\n",
    "plt.plot(decades[1:], mean_change[2], label=test_words[2])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Decades\")\n",
    "plt.ylabel(\"Similarity with Previous Embedding\")\n",
    "plt.title(\"Meaning Shift of Top Three Words Relative to Previous Decade\")\n",
    "plt.scatter([1940, 1930, 1960], [0, 0.77, 0.70], color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this change point detection algorithm, we can simply calculate the degree to which each decade's embedding is similar to the embedding from the previous decade. If the similarity is high between a pair of consecutive decades, this suggests that the word has not experienced a large shift in meaning. Conversely, if we see a dip in cosine similarity in a particular decade, this would tell us that the word has experienced a meaning shift at that point in time, and thus that this is our change point. The red points show change points detected by the algorithm.\n",
    "\n",
    "For the word \"computer\", we see that this happens in 1940s, when it acquires a more digital meaning. The word \"program\" seems to experience a shift around 1960, which is sensible as it occurs after computers become more ubiquitous. Finally, the word \"objectives\" seems to experience its change point in the 1930s. I'm not sure why this would be the case, though it could be the case that the word took on a more militaristic meaning given the prevalence of the World Wars during these decades."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3da9994af9e3093f34be5556416449be7c3908df46ffc29ccdba7bd3fd3a7b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
